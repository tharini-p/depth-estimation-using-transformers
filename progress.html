<!doctype html>
<html lang="en">
<head>
<title>Your Project Name</title>
<meta property="og:title" content=Your Project Name" />
<meta name="twitter:title" content="Your Project Name" />
<meta name="description" content="Your project about your cool topic described right here." />
<meta property="og:description" content="Your project about your cool topic described right here." />
<meta name="twitter:description" content="Your project about your cool topic described right here." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">Monocular Depth Estimation using Vision Transformers</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
    <div class="row">
      <div class="col justify-content-center text-center">
        <h2>What are vision transformers, how has transformers become better at solving depth estimation with single image?</h2>
        <p>Is AI able to solve problem which humans can't naturally?</p>
      </div>
    </div>
    <div class="row">
      <div class="col">


        <h3>Dataset</h3>

        <p>There are two datasets that we tried working. Kitti dataset beting too large we thought of just running the testset once we built the model using other datasets. For training validation dataset of the below models were used for now as the 
            actual training datasets are as large as 84gb.
        </p>
        <h5>DIODE</h5>
        <p>DIODE (Dense Indoor and Outdoor DEpth) is a dataset that contains diverse high-resolution color images with accurate, dense, far-range depth measurements.
             It is the first public dataset to include RGBD images of indoor and outdoor scenes obtained with one sensor suite. The dataset contains an RGB image and a depth image, where the closer objects have blue color and as it goes back the farthest object has red color.</p>
             <table style="width: 100%;margin-bottom: 30px;">
                <tr>
                    <td><img style="width: 100%;height: 100%;" alt="" src="img/img1.png"></img></td>        </tr>
                </table>
 
        <h5>NYU-Depth V2</h5>
         <p> We used the labelled dataset of NYU-2 which is close to 2.8GB. It is comprised of pairs of RGB and Depth frames that have been synchronized and annotated with dense labels for every image. </p>
        
        
        
        
         <table style="width: 100%;margin-bottom: 30px;">
            <tr>
                <td><img style="width: 100%;height: 100%;" alt="" src="img/img2.png"></img></td>        </tr>
            </table>


            <h3>Model Architecture</h3>

            <p>
                As part of our initial analysis of the project we implemented a UNet CNN with a ResNet34 encoder which uses weights
                pre-trained on imagenet. U-Net has two parts an encoder and a decoder. The encoder decoder set up allows us to encode
                the RGB image and get back the same image but in a different form(depth based colored).              

                U-Net has the ability for pixel-level localization and distinguishes unique patterns. Each block of the decoder is connected to the corresponding block of the encoder using Skip connections. 
                This helps it to have more latent spaces than its traditional counterpart and that said, it uses the information right from the input space and all of its intermediate representations to map the sample onto some well-defined latent space and then computing the output from it. These skip connections are experimentally validated in many pieces of research to solve the degradation problem.
            </p>
            <table style="width: 100%;margin-bottom: 30px;">
                <tr>
                    <td><img style="width: 100%;height: 100%;" alt="" src="img/img3.png"></img></td>        </tr>
                </table>
    



      

        <h2>Implementation Details</h2>

        <p> 
          For our dataset, we used <a href='[2]'>NYU-V2 [2]</a> validation dataset and, split the dataset into train and test.
          The folder format of the dataset can be seen <a href='[2]'>here [2]</a>. The training set if further split into 70% train and 30%
          validation datasets. As part of data preprocessing we perform random horizontal flips of the image and normalize the images. We used an 
          image size of 128*128 with 3 RGB channels as input. For hyperparameters, we used 16 batch size, 15 epochs, 0.0001 learning rate.
 
        </p>

        <h2>Findings</h1>

          <p>
            In the beginning we started to understand the Kitti datasets, because the dataset is built using an autonomous driving platform. But due to the 
            dataset being large we decided to train our model on DIODE dataset. To begin with we ran a trial run on a pre-trained <a href='#[5]'>vision transformer</a>
            to understand how well the depth estimation it was able to do with respect to the ground truth.

            Thereafter as part of our first steps, we implemented a pipeline to perform the following

            <ul>
                <li>
                    Split the dataset into validation and train.
                </li>
                <li>
                    Peform preprocessing on the images, normalize and create augmented data.
                </li>
                <li>
                    Train the model for given number of epochs with loss function as the absolute differennce between the true predicction 
                    and the ground truth. The model starts with the pre-trained weights of imagenet.
                </li>
            </ul>

            
            <table style="width: 100%;margin-bottom: 30px;">
                <tr>
                    <td><img style="width: 50%;height: 50%;" alt="" src="img/img4.png"></img>
                        <figcaption>The left column shows the predicted depth map, the centre column shows the ground truth and the 
                            rightmost column shows the input image.
                        </figcaption>
                    </td>        </tr>
                </table>
    
            <p>We can see that for a 15 epoch, using pretrained weights the model does an okay job at least for understanding what is near 
            and what is far. But the model does do a good job at proper segmentation. When there are too many objects in the image the model 
            does not do a good job. We feel self attention and increasing the number of epochs might help improve this. With self-attention the model
            might undertand what pixels might be related in terms ob the object and the orientation in an image.
           </p>
        

        
        <p> We see that the training for a 128*128 vision transformer would require a high computational power, so we are putting a hold on the 
        project for now and trying to get a indepth understanding of Transformers and BERT, and how self-attention has become a game changer. We are 
		planning to utilize BERT to build a Question Answering model trained to answer questions from COVID related research articles.<a href="bert_qa.html">(Click here for details)</a>
		</p>
		
		


      <h2> References</h2>
        <p><a name="[1]">[1]</a> <a href="https://arxiv.org/abs/2103.13413">
          <em>Vision Transformers for Dense Prediction.</em> </a> </p>
        <p><a name="[2]">[2]</a> <a href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">
            https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html</a> </p>
        <p><a name="[3]">[3]</a> <a href="https://gitlab.com/AshwinUnnikrishnan/monocular-depth">
            https://gitlab.com/AshwinUnnikrishnan/monocular-depth</a> </p>
        <p><a name="[4]">[4]<a href="https://medium.com/mlearning-ai/monocular-depth-estimation-using-u-net-6f149fc34077">
            https://medium.com/mlearning-ai/monocular-depth-estimation-using-u-net-6f149fc34077</a> </p>
        <p><a name="[5]">[5]<a href="https://huggingface.co/Intel/dpt-large">
            https://huggingface.co/Intel/dpt-large</a> </p> 
	  
		<h2>Team Members</h2>
		<ul>
		<li><a href="https://www.linkedin.com/in/tharinipadmagirisan/">Tharini Padmagirisan</a></li>
		<li><a href="https://www.linkedin.com/in/ashwinunnikrishnan/">Ashwin Unnikrishnan</a></li>
		</ul>
	  
	<p>
    <a href="bert_qa.html" target="_blank">Click here for our problem statement and plan for COVID-19 QA system using BERT</a>
	</p>
		
	  
	  </div>
      <!--col-->
    </div>
    <!--row -->
  </div> <!-- container -->

  <footer class="nd-pagefooter">
    <div class="row">
      <div class="col-6 col-md text-center">
        <a href="https://cs7150.baulab.info/">About CS 7150</a>
      </div>
    </div>
  </footer>

</body>
<script>
  $(document).on('click', '.clickselect', function (ev) {
    var range = document.createRange();
    range.selectNodeContents(this);
    var sel = window.getSelection();
    sel.removeAllRanges();
    sel.addRange(range);
  });
  // Google analytics below.
  window.dataLayer = window.dataLayer || [];
</script>

</html>
